{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600325998935",
   "display_name": "Python 3.8.4 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math as m\n",
    "import numpy as np\n",
    "import random as r\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nflows.distributions.uniform import BoxUniform\n",
    "from nflows.transforms.base import CompositeTransform\n",
    "from nflows.flows.base import Flow\n",
    "from nflows.distributions.dropout import StochasticDropout\n",
    "from nflows.distributions.dropout import ProbabilityNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This works with any size x\n",
    "def p(x, n_probs):\n",
    "    sums = torch.sum(x, axis=1)\n",
    "    probs = torch.cos(torch.ger(sums, torch.arange(1, n_probs+1, dtype=torch.float32)))**2\n",
    "    norm = torch.sum(probs, axis=1)\n",
    "\n",
    "    for i in range(n_probs):\n",
    "        probs[:,i] /= norm\n",
    "    \n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(n, drop_indices):\n",
    "    n_probs = torch.max(drop_indices) + 1\n",
    "    x = torch.rand(n, drop_indices.shape[0])\n",
    "    probs = p(x, n_probs)\n",
    "\n",
    "    # Pick a prob\n",
    "    probs_cumsum = torch.cumsum(probs, axis=1)\n",
    "\n",
    "    # Tensor with bools that are true when r passes the cumprob\n",
    "    larger_than_cumprob = torch.rand(n,1) < probs_cumsum\n",
    "    # Do the arange trick to find first nonzero\n",
    "    # This is the HIGHEST LABEL FROM DROP_INDICES THAT IS KEPT\n",
    "    selected_index = torch.argmax(larger_than_cumprob*torch.arange(n_probs, 0, -1), axis=1)\n",
    "\n",
    "    '''\n",
    "    print(\"The index of the selected probability\")\n",
    "    print(\"This is also the highest label in drop_indices that is kept\")\n",
    "    print(selected_index)\n",
    "    ''' \n",
    "    \n",
    "    # Find the index of the first true\n",
    "    drop_mask = drop_indices > selected_index[:,None]\n",
    "    x[drop_mask] = 0\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'\\n#Testing the probability net\\n\\ndrop_indices = torch.tensor([0,0,1,1,1,2,3,3,4])\\nn_dims = drop_indices.shape[0]\\nn_probs = torch.max(drop_indices).item() + 1\\nprint(n_probs)\\n\\n#prob_net = ProbabilityNet(n_dims, n_probs, 10*n_probs, 5)\\nprob_net = ProbabilityNet(n_dims, n_probs, 10*n_probs, 5)\\noptimizer = optim.Adam(prob_net.parameters())\\n\\niterations = 10000\\nbatch_size = 128\\n\\nfor i in range(iterations):\\n    x = torch.rand(batch_size, drop_indices.shape[0])\\n    probs_real = p(x, n_probs)\\n    \\n    optimizer.zero_grad()\\n    loss = ((probs_real - prob_net(x))**2).mean()\\n    loss.backward()    \\n    optimizer.step()\\n    print(loss)\\n'"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "'''\n",
    "#Testing the probability net\n",
    "\n",
    "drop_indices = torch.tensor([0,0,1,1,1,2,3,3,4])\n",
    "n_dims = drop_indices.shape[0]\n",
    "n_probs = torch.max(drop_indices).item() + 1\n",
    "print(n_probs)\n",
    "\n",
    "#prob_net = ProbabilityNet(n_dims, n_probs, 10*n_probs, 5)\n",
    "prob_net = ProbabilityNet(n_dims, n_probs, 10*n_probs, 5)\n",
    "optimizer = optim.Adam(prob_net.parameters())\n",
    "\n",
    "iterations = 10000\n",
    "batch_size = 128\n",
    "\n",
    "for i in range(iterations):\n",
    "    x = torch.rand(batch_size, drop_indices.shape[0])\n",
    "    probs_real = p(x, n_probs)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss = ((probs_real - prob_net(x))**2).mean()\n",
    "    loss.backward()    \n",
    "    optimizer.step()\n",
    "    print(loss)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "gBackward>)\ntensor(1.6212, grad_fn=<NegBackward>)\ntensor(1.6102, grad_fn=<NegBackward>)\ntensor(1.6041, grad_fn=<NegBackward>)\ntensor(1.6143, grad_fn=<NegBackward>)\ntensor(1.6153, grad_fn=<NegBackward>)\ntensor(1.5988, grad_fn=<NegBackward>)\ntensor(1.6008, grad_fn=<NegBackward>)\ntensor(1.6028, grad_fn=<NegBackward>)\ntensor(1.5992, grad_fn=<NegBackward>)\ntensor(1.5998, grad_fn=<NegBackward>)\ntensor(1.6191, grad_fn=<NegBackward>)\ntensor(1.6032, grad_fn=<NegBackward>)\ntensor(1.6120, grad_fn=<NegBackward>)\ntensor(1.6139, grad_fn=<NegBackward>)\ntensor(1.6157, grad_fn=<NegBackward>)\ntensor(1.6095, grad_fn=<NegBackward>)\ntensor(1.6144, grad_fn=<NegBackward>)\ntensor(1.6106, grad_fn=<NegBackward>)\ntensor(1.5972, grad_fn=<NegBackward>)\ntensor(1.6051, grad_fn=<NegBackward>)\ntensor(1.6291, grad_fn=<NegBackward>)\ntensor(1.6127, grad_fn=<NegBackward>)\ntensor(1.6142, grad_fn=<NegBackward>)\ntensor(1.6067, grad_fn=<NegBackward>)\ntensor(1.6028, grad_fn=<NegBackward>)\ntensor(1.6156, grad_fn=<NegBackward>)\ntensor(1.6055, grad_fn=<NegBackward>)\ntensor(1.6014, grad_fn=<NegBackward>)\ntensor(1.6091, grad_fn=<NegBackward>)\ntensor(1.6182, grad_fn=<NegBackward>)\ntensor(1.6014, grad_fn=<NegBackward>)\ntensor(1.6200, grad_fn=<NegBackward>)\ntensor(1.6160, grad_fn=<NegBackward>)\ntensor(1.6126, grad_fn=<NegBackward>)\ntensor(1.6139, grad_fn=<NegBackward>)\ntensor(1.6028, grad_fn=<NegBackward>)\ntensor(1.5971, grad_fn=<NegBackward>)\ntensor(1.6130, grad_fn=<NegBackward>)\ntensor(1.5951, grad_fn=<NegBackward>)\ntensor(1.6107, grad_fn=<NegBackward>)\ntensor(1.6109, grad_fn=<NegBackward>)\ntensor(1.6058, grad_fn=<NegBackward>)\ntensor(1.5953, grad_fn=<NegBackward>)\ntensor(1.6043, grad_fn=<NegBackward>)\ntensor(1.6079, grad_fn=<NegBackward>)\ntensor(1.6045, grad_fn=<NegBackward>)\ntensor(1.6065, grad_fn=<NegBackward>)\ntensor(1.6181, grad_fn=<NegBackward>)\ntensor(1.6125, grad_fn=<NegBackward>)\ntensor(1.6088, grad_fn=<NegBackward>)\ntensor(1.6134, grad_fn=<NegBackward>)\ntensor(1.6000, grad_fn=<NegBackward>)\ntensor(1.6051, grad_fn=<NegBackward>)\ntensor(1.6129, grad_fn=<NegBackward>)\ntensor(1.6149, grad_fn=<NegBackward>)\ntensor(1.6102, grad_fn=<NegBackward>)\ntensor(1.6169, grad_fn=<NegBackward>)\ntensor(1.5977, grad_fn=<NegBackward>)\ntensor(1.6061, grad_fn=<NegBackward>)\ntensor(1.6059, grad_fn=<NegBackward>)\ntensor(1.6068, grad_fn=<NegBackward>)\ntensor(1.6022, grad_fn=<NegBackward>)\ntensor(1.6139, grad_fn=<NegBackward>)\ntensor(1.6088, grad_fn=<NegBackward>)\ntensor(1.6122, grad_fn=<NegBackward>)\ntensor(1.5927, grad_fn=<NegBackward>)\ntensor(1.6059, grad_fn=<NegBackward>)\ntensor(1.6101, grad_fn=<NegBackward>)\ntensor(1.6045, grad_fn=<NegBackward>)\ntensor(1.6089, grad_fn=<NegBackward>)\ntensor(1.6015, grad_fn=<NegBackward>)\ntensor(1.6049, grad_fn=<NegBackward>)\ntensor(1.6036, grad_fn=<NegBackward>)\ntensor(1.6007, grad_fn=<NegBackward>)\ntensor(1.6055, grad_fn=<NegBackward>)\ntensor(1.5981, grad_fn=<NegBackward>)\ntensor(1.6103, grad_fn=<NegBackward>)\ntensor(1.6061, grad_fn=<NegBackward>)\ntensor(1.6138, grad_fn=<NegBackward>)\ntensor(1.6063, grad_fn=<NegBackward>)\ntensor(1.6097, grad_fn=<NegBackward>)\ntensor(1.6080, grad_fn=<NegBackward>)\ntensor(1.6064, grad_fn=<NegBackward>)\ntensor(1.6106, grad_fn=<NegBackward>)\ntensor(1.6095, grad_fn=<NegBackward>)\ntensor(1.6112, grad_fn=<NegBackward>)\ntensor(1.6054, grad_fn=<NegBackward>)\ntensor(1.6052, grad_fn=<NegBackward>)\ntensor(1.6053, grad_fn=<NegBackward>)\ntensor(1.6088, grad_fn=<NegBackward>)\ntensor(1.6110, grad_fn=<NegBackward>)\ntensor(1.6120, grad_fn=<NegBackward>)\ntensor(1.6147, grad_fn=<NegBackward>)\ntensor(1.6088, grad_fn=<NegBackward>)\ntensor(1.5908, grad_fn=<NegBackward>)\ntensor(1.6157, grad_fn=<NegBackward>)\ntensor(1.6160, grad_fn=<NegBackward>)\ntensor(1.5979, grad_fn=<NegBackward>)\ntensor(1.5978, grad_fn=<NegBackward>)\ntensor(1.6001, grad_fn=<NegBackward>)\ntensor(1.6190, grad_fn=<NegBackward>)\ntensor(1.6147, grad_fn=<NegBackward>)\ntensor(1.6092, grad_fn=<NegBackward>)\ntensor(1.6105, grad_fn=<NegBackward>)\ntensor(1.6052, grad_fn=<NegBackward>)\ntensor(1.6040, grad_fn=<NegBackward>)\ntensor(1.6179, grad_fn=<NegBackward>)\ntensor(1.6082, grad_fn=<NegBackward>)\ntensor(1.6038, grad_fn=<NegBackward>)\ntensor(1.6079, grad_fn=<NegBackward>)\ntensor(1.6190, grad_fn=<NegBackward>)\ntensor(1.6118, grad_fn=<NegBackward>)\ntensor(1.6105, grad_fn=<NegBackward>)\ntensor(1.6133, grad_fn=<NegBackward>)\ntensor(1.5988, grad_fn=<NegBackward>)\ntensor(1.6101, grad_fn=<NegBackward>)\ntensor(1.5973, grad_fn=<NegBackward>)\ntensor(1.6243, grad_fn=<NegBackward>)\ntensor(1.5976, grad_fn=<NegBackward>)\ntensor(1.6051, grad_fn=<NegBackward>)\ntensor(1.6013, grad_fn=<NegBackward>)\ntensor(1.6101, grad_fn=<NegBackward>)\ntensor(1.6085, grad_fn=<NegBackward>)\ntensor(1.6081, grad_fn=<NegBackward>)\ntensor(1.6061, grad_fn=<NegBackward>)\ntensor(1.6130, grad_fn=<NegBackward>)\ntensor(1.6116, grad_fn=<NegBackward>)\ntensor(1.6129, grad_fn=<NegBackward>)\ntensor(1.6007, grad_fn=<NegBackward>)\ntensor(1.5893, grad_fn=<NegBackward>)\ntensor(1.6024, grad_fn=<NegBackward>)\ntensor(1.6059, grad_fn=<NegBackward>)\ntensor(1.5937, grad_fn=<NegBackward>)\ntensor(1.6144, grad_fn=<NegBackward>)\ntensor(1.6223, grad_fn=<NegBackward>)\ntensor(1.6130, grad_fn=<NegBackward>)\ntensor(1.6097, grad_fn=<NegBackward>)\ntensor(1.5996, grad_fn=<NegBackward>)\ntensor(1.6090, grad_fn=<NegBackward>)\ntensor(1.6113, grad_fn=<NegBackward>)\ntensor(1.6053, grad_fn=<NegBackward>)\ntensor(1.6184, grad_fn=<NegBackward>)\ntensor(1.6150, grad_fn=<NegBackward>)\ntensor(1.6126, grad_fn=<NegBackward>)\ntensor(1.6071, grad_fn=<NegBackward>)\ntensor(1.6130, grad_fn=<NegBackward>)\ntensor(1.6135, grad_fn=<NegBackward>)\ntensor(1.6079, grad_fn=<NegBackward>)\ntensor(1.6221, grad_fn=<NegBackward>)\ntensor(1.6094, grad_fn=<NegBackward>)\ntensor(1.6064, grad_fn=<NegBackward>)\ntensor(1.6233, grad_fn=<NegBackward>)\ntensor(1.6006, grad_fn=<NegBackward>)\ntensor(1.6175, grad_fn=<NegBackward>)\ntensor(1.6038, grad_fn=<NegBackward>)\ntensor(1.6207, grad_fn=<NegBackward>)\ntensor(1.6119, grad_fn=<NegBackward>)\ntensor(1.6165, grad_fn=<NegBackward>)\ntensor(1.6119, grad_fn=<NegBackward>)\ntensor(1.6148, grad_fn=<NegBackward>)\ntensor(1.6119, grad_fn=<NegBackward>)\ntensor(1.6038, grad_fn=<NegBackward>)\ntensor(1.6016, grad_fn=<NegBackward>)\ntensor(1.6060, grad_fn=<NegBackward>)\ntensor(1.6027, grad_fn=<NegBackward>)\ntensor(1.6187, grad_fn=<NegBackward>)\ntensor(1.6158, grad_fn=<NegBackward>)\ntensor(1.6013, grad_fn=<NegBackward>)\ntensor(1.6102, grad_fn=<NegBackward>)\ntensor(1.6159, grad_fn=<NegBackward>)\ntensor(1.6041, grad_fn=<NegBackward>)\ntensor(1.6134, grad_fn=<NegBackward>)\ntensor(1.6156, grad_fn=<NegBackward>)\ntensor(1.6463, grad_fn=<NegBackward>)\ntensor(1.6181, grad_fn=<NegBackward>)\ntensor(1.6000, grad_fn=<NegBackward>)\ntensor(1.6136, grad_fn=<NegBackward>)\ntensor(1.6078, grad_fn=<NegBackward>)\ntensor(1.6119, grad_fn=<NegBackward>)\ntensor(1.6226, grad_fn=<NegBackward>)\ntensor(1.6310, grad_fn=<NegBackward>)\ntensor(1.6133, grad_fn=<NegBackward>)\ntensor(1.6241, grad_fn=<NegBackward>)\ntensor(1.6031, grad_fn=<NegBackward>)\ntensor(1.6168, grad_fn=<NegBackward>)\ntensor(1.6255, grad_fn=<NegBackward>)\ntensor(1.6108, grad_fn=<NegBackward>)\ntensor(1.6057, grad_fn=<NegBackward>)\ntensor(1.5949, grad_fn=<NegBackward>)\ntensor(1.5918, grad_fn=<NegBackward>)\ntensor(1.6019, grad_fn=<NegBackward>)\ntensor(1.6030, grad_fn=<NegBackward>)\ntensor(1.6034, grad_fn=<NegBackward>)\ntensor(1.6002, grad_fn=<NegBackward>)\ntensor(1.6055, grad_fn=<NegBackward>)\ntensor(1.6039, grad_fn=<NegBackward>)\ntensor(1.6082, grad_fn=<NegBackward>)\ntensor(1.6328, grad_fn=<NegBackward>)\ntensor(1.5966, grad_fn=<NegBackward>)\ntensor(1.6376, grad_fn=<NegBackward>)\ntensor(1.5966, grad_fn=<NegBackward>)\ntensor(1.6037, grad_fn=<NegBackward>)\ntensor(1.6321, grad_fn=<NegBackward>)\ntensor(1.6055, grad_fn=<NegBackward>)\ntensor(1.6126, grad_fn=<NegBackward>)\ntensor(1.6189, grad_fn=<NegBackward>)\ntensor(1.6070, grad_fn=<NegBackward>)\ntensor(1.6049, grad_fn=<NegBackward>)\ntensor(1.6210, grad_fn=<NegBackward>)\ntensor(1.6093, grad_fn=<NegBackward>)\ntensor(1.6038, grad_fn=<NegBackward>)\ntensor(1.5945, grad_fn=<NegBackward>)\ntensor(1.6036, grad_fn=<NegBackward>)\ntensor(1.6098, grad_fn=<NegBackward>)\ntensor(1.6118, grad_fn=<NegBackward>)\ntensor(1.6265, grad_fn=<NegBackward>)\ntensor(1.6143, grad_fn=<NegBackward>)\ntensor(1.6079, grad_fn=<NegBackward>)\ntensor(1.5956, grad_fn=<NegBackward>)\ntensor(1.5943, grad_fn=<NegBackward>)\ntensor(1.6264, grad_fn=<NegBackward>)\ntensor(1.6070, grad_fn=<NegBackward>)\ntensor(1.6062, grad_fn=<NegBackward>)\ntensor(1.6043, grad_fn=<NegBackward>)\ntensor(1.6054, grad_fn=<NegBackward>)\ntensor(1.5857, grad_fn=<NegBackward>)\ntensor(1.5983, grad_fn=<NegBackward>)\ntensor(1.5998, grad_fn=<NegBackward>)\ntensor(1.6018, grad_fn=<NegBackward>)\ntensor(1.5986, grad_fn=<NegBackward>)\ntensor(1.6213, grad_fn=<NegBackward>)\ntensor(1.6057, grad_fn=<NegBackward>)\ntensor(1.5975, grad_fn=<NegBackward>)\ntensor(1.5998, grad_fn=<NegBackward>)\ntensor(1.6012, grad_fn=<NegBackward>)\ntensor(1.6208, grad_fn=<NegBackward>)\ntensor(1.6303, grad_fn=<NegBackward>)\ntensor(1.6353, grad_fn=<NegBackward>)\ntensor(1.6049, grad_fn=<NegBackward>)\ntensor(1.5945, grad_fn=<NegBackward>)\ntensor(1.6287, grad_fn=<NegBackward>)\ntensor(1.6124, grad_fn=<NegBackward>)\ntensor(1.6183, grad_fn=<NegBackward>)\ntensor(1.6175, grad_fn=<NegBackward>)\ntensor(1.5853, grad_fn=<NegBackward>)\ntensor(1.6118, grad_fn=<NegBackward>)\ntensor(1.6174, grad_fn=<NegBackward>)\ntensor(1.6121, grad_fn=<NegBackward>)\ntensor(1.6152, grad_fn=<NegBackward>)\ntensor(1.6096, grad_fn=<NegBackward>)\ntensor(1.6111, grad_fn=<NegBackward>)\ntensor(1.6251, grad_fn=<NegBackward>)\ntensor(1.6103, grad_fn=<NegBackward>)\ntensor(1.5879, grad_fn=<NegBackward>)\ntensor(1.5963, grad_fn=<NegBackward>)\ntensor(1.6131, grad_fn=<NegBackward>)\ntensor(1.6266, grad_fn=<NegBackward>)\ntensor(1.6058, grad_fn=<NegBackward>)\ntensor(1.6041, grad_fn=<NegBackward>)\ntensor(1.5981, grad_fn=<NegBackward>)\ntensor(1.6051, grad_fn=<NegBackward>)\ntensor(1.6207, grad_fn=<NegBackward>)\ntensor(1.6152, grad_fn=<NegBackward>)\ntensor(1.6237, grad_fn=<NegBackward>)\ntensor(1.6194, grad_fn=<NegBackward>)\ntensor(1.6104, grad_fn=<NegBackward>)\ntensor(1.6200, grad_fn=<NegBackward>)\ntensor(1.6066, grad_fn=<NegBackward>)\ntensor(1.6304, grad_fn=<NegBackward>)\ntensor(1.6088, grad_fn=<NegBackward>)\ntensor(1.6108, grad_fn=<NegBackward>)\ntensor(1.6059, grad_fn=<NegBackward>)\ntensor(1.5995, grad_fn=<NegBackward>)\ntensor(1.6202, grad_fn=<NegBackward>)\ntensor(1.5996, grad_fn=<NegBackward>)\ntensor(1.6197, grad_fn=<NegBackward>)\ntensor(1.5841, grad_fn=<NegBackward>)\ntensor(1.6037, grad_fn=<NegBackward>)\ntensor(1.5947, grad_fn=<NegBackward>)\ntensor(1.6128, grad_fn=<NegBackward>)\ntensor(1.6172, grad_fn=<NegBackward>)\ntensor(1.6180, grad_fn=<NegBackward>)\ntensor(1.6065, grad_fn=<NegBackward>)\ntensor(1.6002, grad_fn=<NegBackward>)\ntensor(1.6170, grad_fn=<NegBackward>)\ntensor(1.6130, grad_fn=<NegBackward>)\ntensor(1.6022, grad_fn=<NegBackward>)\ntensor(1.6371, grad_fn=<NegBackward>)\ntensor(1.6071, grad_fn=<NegBackward>)\ntensor(1.6130, grad_fn=<NegBackward>)\ntensor(1.6057, grad_fn=<NegBackward>)\ntensor(1.6062, grad_fn=<NegBackward>)\ntensor(1.6144, grad_fn=<NegBackward>)\ntensor(1.6226, grad_fn=<NegBackward>)\ntensor(1.6187, grad_fn=<NegBackward>)\ntensor(1.6021, grad_fn=<NegBackward>)\ntensor(1.6245, grad_fn=<NegBackward>)\ntensor(1.6177, grad_fn=<NegBackward>)\ntensor(1.6098, grad_fn=<NegBackward>)\ntensor(1.6159, grad_fn=<NegBackward>)\ntensor(1.6210, grad_fn=<NegBackward>)\ntensor(1.6062, grad_fn=<NegBackward>)\ntensor(1.5949, grad_fn=<NegBackward>)\ntensor(1.5971, grad_fn=<NegBackward>)\ntensor(1.5955, grad_fn=<NegBackward>)\ntensor(1.6324, grad_fn=<NegBackward>)\ntensor(1.6249, grad_fn=<NegBackward>)\ntensor(1.6130, grad_fn=<NegBackward>)\ntensor(1.6065, grad_fn=<NegBackward>)\ntensor(1.6165, grad_fn=<NegBackward>)\ntensor(1.6099, grad_fn=<NegBackward>)\ntensor(1.6250, grad_fn=<NegBackward>)\ntensor(1.6088, grad_fn=<NegBackward>)\ntensor(1.6218, grad_fn=<NegBackward>)\ntensor(1.6139, grad_fn=<NegBackward>)\ntensor(1.6005, grad_fn=<NegBackward>)\ntensor(1.6149, grad_fn=<NegBackward>)\ntensor(1.5995, grad_fn=<NegBackward>)\ntensor(1.6009, grad_fn=<NegBackward>)\ntensor(1.5980, grad_fn=<NegBackward>)\ntensor(1.5949, grad_fn=<NegBackward>)\ntensor(1.6152, grad_fn=<NegBackward>)\ntensor(1.6103, grad_fn=<NegBackward>)\ntensor(1.5979, grad_fn=<NegBackward>)\ntensor(1.6137, grad_fn=<NegBackward>)\ntensor(1.6003, grad_fn=<NegBackward>)\ntensor(1.6095, grad_fn=<NegBackward>)\ntensor(1.6175, grad_fn=<NegBackward>)\ntensor(1.6180, grad_fn=<NegBackward>)\ntensor(1.6231, grad_fn=<NegBackward>)\ntensor(1.5967, grad_fn=<NegBackward>)\ntensor(1.6140, grad_fn=<NegBackward>)\ntensor(1.6060, grad_fn=<NegBackward>)\ntensor(1.5907, grad_fn=<NegBackward>)\ntensor(1.6035, grad_fn=<NegBackward>)\ntensor(1.6197, grad_fn=<NegBackward>)\ntensor(1.6110, grad_fn=<NegBackward>)\ntensor(1.6129, grad_fn=<NegBackward>)\ntensor(1.6185, grad_fn=<NegBackward>)\ntensor(1.6138, grad_fn=<NegBackward>)\ntensor(1.6276, grad_fn=<NegBackward>)\ntensor(1.6191, grad_fn=<NegBackward>)\ntensor(1.6209, grad_fn=<NegBackward>)\ntensor(1.6301, grad_fn=<NegBackward>)\ntensor(1.6040, grad_fn=<NegBackward>)\ntensor(1.6180, grad_fn=<NegBackward>)\ntensor(1.6233, grad_fn=<NegBackward>)\ntensor(1.6082, grad_fn=<NegBackward>)\ntensor(1.6041, grad_fn=<NegBackward>)\ntensor(1.6166, grad_fn=<NegBackward>)\ntensor(1.6214, grad_fn=<NegBackward>)\ntensor(1.6061, grad_fn=<NegBackward>)\ntensor(1.5999, grad_fn=<NegBackward>)\ntensor(1.6259, grad_fn=<NegBackward>)\ntensor(1.6147, grad_fn=<NegBackward>)\ntensor(1.6266, grad_fn=<NegBackward>)\ntensor(1.6022, grad_fn=<NegBackward>)\ntensor(1.6173, grad_fn=<NegBackward>)\ntensor(1.6065, grad_fn=<NegBackward>)\ntensor(1.6175, grad_fn=<NegBackward>)\ntensor(1.6199, grad_fn=<NegBackward>)\ntensor(1.6141, grad_fn=<NegBackward>)\ntensor(1.6178, grad_fn=<NegBackward>)\ntensor(1.6161, grad_fn=<NegBackward>)\ntensor(1.5995, grad_fn=<NegBackward>)\ntensor(1.6146, grad_fn=<NegBackward>)\ntensor(1.6105, grad_fn=<NegBackward>)\ntensor(1.6119, grad_fn=<NegBackward>)\ntensor(1.6134, grad_fn=<NegBackward>)\ntensor(1.6098, grad_fn=<NegBackward>)\ntensor(1.6130, grad_fn=<NegBackward>)\ntensor(1.6093, grad_fn=<NegBackward>)\ntensor(1.6113, grad_fn=<NegBackward>)\ntensor(1.6087, grad_fn=<NegBackward>)\ntensor(1.6082, grad_fn=<NegBackward>)\ntensor(1.6074, grad_fn=<NegBackward>)\ntensor(1.6130, grad_fn=<NegBackward>)\ntensor(1.6155, grad_fn=<NegBackward>)\ntensor(1.6080, grad_fn=<NegBackward>)\ntensor(1.6027, grad_fn=<NegBackward>)\ntensor(1.6104, grad_fn=<NegBackward>)\ntensor(1.6140, grad_fn=<NegBackward>)\ntensor(1.6198, grad_fn=<NegBackward>)\ntensor(1.6126, grad_fn=<NegBackward>)\ntensor(1.6292, grad_fn=<NegBackward>)\ntensor(1.6228, grad_fn=<NegBackward>)\ntensor(1.6127, grad_fn=<NegBackward>)\ntensor(1.6045, grad_fn=<NegBackward>)\ntensor(1.5993, grad_fn=<NegBackward>)\ntensor(1.6073, grad_fn=<NegBackward>)\ntensor(1.6131, grad_fn=<NegBackward>)\ntensor(1.6262, grad_fn=<NegBackward>)\ntensor(1.6232, grad_fn=<NegBackward>)\ntensor(1.6109, grad_fn=<NegBackward>)\ntensor(1.6019, grad_fn=<NegBackward>)\ntensor(1.6016, grad_fn=<NegBackward>)\ntensor(1.6109, grad_fn=<NegBackward>)\ntensor(1.5884, grad_fn=<NegBackward>)\ntensor(1.6004, grad_fn=<NegBackward>)\ntensor(1.6138, grad_fn=<NegBackward>)\ntensor(1.6118, grad_fn=<NegBackward>)\ntensor(1.6135, grad_fn=<NegBackward>)\ntensor(1.6047, grad_fn=<NegBackward>)\ntensor(1.6059, grad_fn=<NegBackward>)\ntensor(1.6000, grad_fn=<NegBackward>)\ntensor(1.6060, grad_fn=<NegBackward>)\ntensor(1.6113, grad_fn=<NegBackward>)\ntensor(1.6129, grad_fn=<NegBackward>)\ntensor(1.6076, grad_fn=<NegBackward>)\ntensor(1.6077, grad_fn=<NegBackward>)\ntensor(1.6068, grad_fn=<NegBackward>)\ntensor(1.5977, grad_fn=<NegBackward>)\ntensor(1.6065, grad_fn=<NegBackward>)\ntensor(1.6124, grad_fn=<NegBackward>)\ntensor(1.6142, grad_fn=<NegBackward>)\ntensor(1.6138, grad_fn=<NegBackward>)\ntensor(1.6100, grad_fn=<NegBackward>)\ntensor(1.6155, grad_fn=<NegBackward>)\ntensor(1.6050, grad_fn=<NegBackward>)\ntensor(1.6103, grad_fn=<NegBackward>)\ntensor(1.6116, grad_fn=<NegBackward>)\ntensor(1.6141, grad_fn=<NegBackward>)\ntensor(1.6214, grad_fn=<NegBackward>)\ntensor(1.6130, grad_fn=<NegBackward>)\ntensor(1.6028, grad_fn=<NegBackward>)\ntensor(1.6058, grad_fn=<NegBackward>)\ntensor(1.6151, grad_fn=<NegBackward>)\ntensor(1.6154, grad_fn=<NegBackward>)\ntensor(1.6141, grad_fn=<NegBackward>)\ntensor(1.6114, grad_fn=<NegBackward>)\ntensor(1.6132, grad_fn=<NegBackward>)\ntensor(1.6099, grad_fn=<NegBackward>)\ntensor(1.6135, grad_fn=<NegBackward>)\ntensor(1.6096, grad_fn=<NegBackward>)\ntensor(1.6088, grad_fn=<NegBackward>)\ntensor(1.6116, grad_fn=<NegBackward>)\ntensor(1.6144, grad_fn=<NegBackward>)\ntensor(1.6131, grad_fn=<NegBackward>)\ntensor(1.6118, grad_fn=<NegBackward>)\ntensor(1.6117, grad_fn=<NegBackward>)\ntensor(1.6007, grad_fn=<NegBackward>)\ntensor(1.6056, grad_fn=<NegBackward>)\ntensor(1.6067, grad_fn=<NegBackward>)\ntensor(1.6109, grad_fn=<NegBackward>)\ntensor(1.6076, grad_fn=<NegBackward>)\ntensor(1.6111, grad_fn=<NegBackward>)\ntensor(1.6139, grad_fn=<NegBackward>)\ntensor(1.6086, grad_fn=<NegBackward>)\ntensor(1.6135, grad_fn=<NegBackward>)\ntensor(1.6153, grad_fn=<NegBackward>)\ntensor(1.6177, grad_fn=<NegBackward>)\ntensor(1.6208, grad_fn=<NegBackward>)\ntensor(1.6108, grad_fn=<NegBackward>)\ntensor(1.6138, grad_fn=<NegBackward>)\ntensor(1.6168, grad_fn=<NegBackward>)\ntensor(1.6112, grad_fn=<NegBackward>)\ntensor(1.6051, grad_fn=<NegBackward>)\ntensor(1.6103, grad_fn=<NegBackward>)\ntensor(1.6110, grad_fn=<NegBackward>)\ntensor(1.6138, grad_fn=<NegBackward>)\ntensor(1.6122, grad_fn=<NegBackward>)\ntensor(1.6106, grad_fn=<NegBackward>)\ntensor(1.6079, grad_fn=<NegBackward>)\ntensor(1.6041, grad_fn=<NegBackward>)\ntensor(1.6166, grad_fn=<NegBackward>)\ntensor(1.6077, grad_fn=<NegBackward>)\ntensor(1.6164, grad_fn=<NegBackward>)\ntensor(1.6023, grad_fn=<NegBackward>)\ntensor(1.6181, grad_fn=<NegBackward>)\ntensor(1.6090, grad_fn=<NegBackward>)\ntensor(1.6053, grad_fn=<NegBackward>)\ntensor(1.6033, grad_fn=<NegBackward>)\ntensor(1.6186, grad_fn=<NegBackward>)\ntensor(1.6087, grad_fn=<NegBackward>)\ntensor(1.6157, grad_fn=<NegBackward>)\ntensor(1.6102, grad_fn=<NegBackward>)\ntensor(1.6090, grad_fn=<NegBackward>)\ntensor(1.6196, grad_fn=<NegBackward>)\ntensor(1.6161, grad_fn=<NegBackward>)\ntensor(1.6175, grad_fn=<NegBackward>)\ntensor(1.6160, grad_fn=<NegBackward>)\ntensor(1.6128, grad_fn=<NegBackward>)\ntensor(1.6073, grad_fn=<NegBackward>)\ntensor(1.6099, grad_fn=<NegBackward>)\ntensor(1.6122, grad_fn=<NegBackward>)\ntensor(1.6223, grad_fn=<NegBackward>)\ntensor(1.6010, grad_fn=<NegBackward>)\ntensor(1.6127, grad_fn=<NegBackward>)\ntensor(1.6146, grad_fn=<NegBackward>)\ntensor(1.6034, grad_fn=<NegBackward>)\ntensor(1.6077, grad_fn=<NegBackward>)\ntensor(1.6092, grad_fn=<NegBackward>)\ntensor(1.6063, grad_fn=<NegBackward>)\ntensor(1.6051, grad_fn=<NegBackward>)\ntensor(1.6131, grad_fn=<NegBackward>)\ntensor(1.6084, grad_fn=<NegBackward>)\ntensor(1.6017, grad_fn=<NegBackward>)\ntensor(1.6069, grad_fn=<NegBackward>)\ntensor(1.6054, grad_fn=<NegBackward>)\ntensor(1.6077, grad_fn=<NegBackward>)\ntensor(1.6042, grad_fn=<NegBackward>)\ntensor(1.6046, grad_fn=<NegBackward>)\ntensor(1.6100, grad_fn=<NegBackward>)\ntensor(1.6064, grad_fn=<NegBackward>)\ntensor(1.6107, grad_fn=<NegBackward>)\ntensor(1.6093, grad_fn=<NegBackward>)\ntensor(1.6096, grad_fn=<NegBackward>)\ntensor(1.6135, grad_fn=<NegBackward>)\ntensor(1.6158, grad_fn=<NegBackward>)\ntensor(1.6115, grad_fn=<NegBackward>)\ntensor(1.6088, grad_fn=<NegBackward>)\ntensor(1.6073, grad_fn=<NegBackward>)\ntensor(1.6015, grad_fn=<NegBackward>)\ntensor(1.6149, grad_fn=<NegBackward>)\ntensor(1.6092, grad_fn=<NegBackward>)\ntensor(1.6078, grad_fn=<NegBackward>)\ntensor(1.6187, grad_fn=<NegBackward>)\ntensor(1.6043, grad_fn=<NegBackward>)\ntensor(1.6104, grad_fn=<NegBackward>)\ntensor(1.6066, grad_fn=<NegBackward>)\ntensor(1.6037, grad_fn=<NegBackward>)\ntensor(1.6082, grad_fn=<NegBackward>)\ntensor(1.6050, grad_fn=<NegBackward>)\ntensor(1.6068, grad_fn=<NegBackward>)\ntensor(1.6128, grad_fn=<NegBackward>)\ntensor(1.6041, grad_fn=<NegBackward>)\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-1ce4cd9a3473>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mstoch_drop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/nflowsclone/nflows/nflows/distributions/base.py\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, inputs, context)\u001b[0m\n\u001b[1;32m     38\u001b[0m                     \u001b[0;34m\"Number of input items must be equal to number of context items.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                 )\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/nflowsclone/nflows/nflows/distributions/dropout.py\u001b[0m in \u001b[0;36m_log_prob\u001b[0;34m(self, inputs, context)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0minputs_filled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs_filled\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_zero_elements\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# shape = (n_batch, n_MC, dim_data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;31m# Get dropout likelihoods from net\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prob_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_filled\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# shape = (n_batch, n_probs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0;31m# Take a mean over the n_MC dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mmean_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/nflowsclone/nflows/nflows/distributions/dropout.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# Relu on the first n-1 layers, softmax on the last\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1676\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1677\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1678\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1679\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1680\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "iterations = 10000\n",
    "batch_size = 128\n",
    "\n",
    "drop_indices = torch.tensor([0,0,1,1,1,2,3,3,4])\n",
    "stoch_drop = StochasticDropout(drop_indices)\n",
    "optimizer = optim.Adam(stoch_drop.parameters())\n",
    "\n",
    "for i in range(iterations):\n",
    "    x = generate(batch_size, drop_indices)\n",
    "    loss = -stoch_drop.log_prob(x).mean()\n",
    "    loss.backward()    \n",
    "    optimizer.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'iteration' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-4b18a0b57f1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'iteration' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(iteration):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dist = BoxUniform(torch.zeros(4), torch.ones(4))\n",
    "transforms = []\n",
    "transforms.append(StochasticDropout(torch.tensor([1,2,3,4]), hidden_layers=5))\n",
    "transform = CompositeTransform(transforms)\n",
    "flow = Flow(transform, base_dist)\n",
    "optimizer = optim.Adam(flow.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "generate() missing 1 required positional argument: 'drop_indices'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-7e11c5529944>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: generate() missing 1 required positional argument: 'drop_indices'"
     ]
    }
   ],
   "source": [
    "n_iterations = 10000\n",
    "batch_size = 512\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    x = generate(batch_size)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = -flow.log_prob(inputs=x).mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'x_data' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-6cafeef91c12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mn_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_data' is not defined"
     ]
    }
   ],
   "source": [
    "n_epochs = 5000\n",
    "batch_size = 1\n",
    "n_batches = m.ceil(x_data.shape[0]/batch_size)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    permutation = torch.randperm(x_data.shape[0])    \n",
    "\n",
    "    # Loop over batches\n",
    "    cum_loss = 0\n",
    "    for batch in range(n_batches):\n",
    "        # Set up the batch\n",
    "        batch_begin = batch*batch_size\n",
    "        batch_end   = min( (batch+1)*batch_size, x_data.shape[0] )\n",
    "        indices = permutation[batch_begin:batch_end]\n",
    "        batch_x = x_data[indices]\n",
    "        # Take a step\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = -flow.log_prob(inputs=batch_x).mean()\n",
    "        loss.backward()    \n",
    "        optimizer.step()\n",
    "    \n",
    "        # Compute cumulative loss\n",
    "        cum_loss = (cum_loss*batch + loss.item())/(batch+1)\n",
    "        \n",
    "    print(\"batch = \",batch+1, \"/\", n_batches, \"loss = \", cum_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleAttributeError",
     "evalue": "'StochasticDropout' object has no attribute 'inverse'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleAttributeError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-5b175d9a0093>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mx_flow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/nflowsclone/nflows/nflows/distributions/base.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, num_samples, context, batch_size)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/nflowsclone/nflows/nflows/flows/base.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, num_samples, context)\u001b[0m\n\u001b[1;32m     52\u001b[0m             )\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedded_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0membedded_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/nflowsclone/nflows/nflows/transforms/base.py\u001b[0m in \u001b[0;36minverse\u001b[0;34m(self, inputs, context)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mfuncs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transforms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cascade\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuncs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/nflowsclone/nflows/nflows/transforms/base.py\u001b[0m in \u001b[0;36m_cascade\u001b[0;34m(inputs, funcs, context)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mtotal_logabsdet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfuncs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogabsdet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mtotal_logabsdet\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlogabsdet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/nflowsclone/nflows/nflows/transforms/base.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mfuncs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transforms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cascade\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuncs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m         raise ModuleAttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m    772\u001b[0m             type(self).__name__, name))\n\u001b[1;32m    773\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleAttributeError\u001b[0m: 'StochasticDropout' object has no attribute 'inverse'"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    x_flow = flow.sample(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'x_data' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-22d52798fe88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_nonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhisttype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'stepfilled'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medgecolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"black\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfacecolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"lightgray\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdensity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_nonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_flow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medgecolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"red\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhisttype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"step\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdensity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_data' is not defined"
     ]
    }
   ],
   "source": [
    "plt.hist(np.count_nonzero(x_data.numpy(), axis=1), np.linspace(-0.5,4.5,6), histtype='stepfilled', edgecolor=\"black\", facecolor=\"lightgray\", density=True)\n",
    "plt.hist(np.count_nonzero(x_flow.numpy(), axis=1), np.linspace(-0.5,4.5,6), edgecolor=\"red\", histtype=\"step\", density=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_random = torch.rand(10,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "p() missing 1 required positional argument: 'n_probs'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-ab4e3ad264a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_random\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: p() missing 1 required positional argument: 'n_probs'"
     ]
    }
   ],
   "source": [
    "p(x_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (10) must match the size of tensor b (5) at non-singleton dimension 1",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-9ac66c5402be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-201f9714ef89>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(n)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprobs_cumsum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mprobs_cumsum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (10) must match the size of tensor b (5) at non-singleton dimension 1"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = StochasticDropout(torch.tensor([1,2,3,4]), hidden_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleAttributeError",
     "evalue": "'StochasticDropout' object has no attribute 'inverse'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleAttributeError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-df89582992a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m         raise ModuleAttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m    772\u001b[0m             type(self).__name__, name))\n\u001b[1;32m    773\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleAttributeError\u001b[0m: 'StochasticDropout' object has no attribute 'inverse'"
     ]
    }
   ],
   "source": [
    "a = test.inverse(torch.rand(10,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}